{"/":{"title":"ðŸª´ Nam's Notes.","content":"\nHi, I'm Nam. I do stuffs with computer and sometimes write about it here. I'm from Danang, VN. Currently I ship code at Personio.\n\nIn addition to reading books, playing Dota2, I'm deeply passionate about distributed system and [leetcode](https://leetcode.com/) everyday.\n\nThis is my daily notes, I'm using [Obsidian](https://obsidian.md/) as my knowledge base.\n\n[All my notes](/notes)\n","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/AVL-Tree":{"title":"AVL Tree","content":"","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Aggregates":{"title":"Aggregates","content":"\n#distributed-system #pattern ","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Bottle-neck":{"title":"Bottle neck","content":"","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Code-review":{"title":"Code review","content":"![[notes/images/CFFDADEE-EB19-496B-B9A2-BF726A97E05E.jpeg]]","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Database-per-Service":{"title":"Database per Service","content":"#distributed-system #pattern ","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Debezium-Connectors":{"title":"Debezium Connectors","content":"","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Design-data-intensive-application":{"title":"Design data intensive application","content":"\n#book #database\n\n[[SSTable]]\n\n[[Replication]]","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Distributed-Systems":{"title":"Distributed Systems","content":"\n[[read-after-write]]","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Domain-Events":{"title":"Domain Events","content":"#distributed-system #pattern ","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Dual-writes":{"title":"Dual writes","content":"\n#dual-writes\n### Resources\n- https://developers.redhat.com/articles/2021/07/30/avoiding-dual-writes-event-driven-applications#","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/ECEs-Entity-Changed-Events":{"title":"ECE","content":"\n#### Production and consumption\n\nIn order to produce ECEs, one need to:\n- Create an ECE topic following the naming convention\n- In the producer service, leverage the ebdr-outbox library\n- Create a connector\nIn order to consume ECEs, one need to:\n- Give proper permission to the AWS IAM role associated with the service which is going to consume ECEs\n- in consumer service, leverage ebdr-consumer library","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Event-Sourcing":{"title":"Event Sourcing","content":"\n#distributed-system #pattern ","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Events":{"title":"Events","content":"\n### Event Types\n\n#### Change Data Capture Events (CDC)\n- refers to the process of capturing changes made to a database\n- Row level changes or schema level changes\n- CDC Events **MUST NOT** be consumed by consumers (ÂµS) outside the Bounded Context which owns the data\n- For example: Company data is owned and served by Monolith. Extracting all logic into a separate ÂµS requires data to be migrated as well, since every service maintains its own datastore. Having CDC event streaming in place would allow the ÂµS to migrate the changes to its datastore.\n- Visibility: CDC events are private messages, and hence the events must be produced to/consumed from private topics. Kafka allows configuring ACL for restricted accesses.\n#### Entity Changed Events (ECE)\n[[ECEs - Entity Changed Events]]\n- represent a data structure with the intent of presenting changes in a entity. This at first might sound similar to CDC, but conceptually they're different. ECE  Events define a public contract, which can be used by Foreign Bounded Contexts. They're used primarily for **Data Replication** between different systems. For the consumers that listen to ECEs, the guarantee is that they will receive the latest state of an entity, but might not receive all the intermediates states, due to #compaction.\n\n##### How is different from CDC?\n- CDC could be the basis of Entity Changed Events, there are scenarios where the produced may directly produce such an event without need for CDC.\n- CDC events are granular as they can expose Table/Schema/Row Level changes.\n- ECEs can communicate changes on Domain Entity, i.e. `Company` could be a domain model and `CompanyChangedEvent` will present changes on that model. The important point here is that the persistence model could span several tables.\n\n#### Domain Events\nDomain Events are described as something that happened in the domain. Just like Entity Changed Events, domain events also define a public contract. Such events typically occur regardless of whether or to what extent the domain is implemented in software development. They are also independent of technologies.\n- An employee is hired\n- An employee is fired\n- An candidate rejected an offer.\n\nDomain events are relevant both within a bounded context and across bounded context for implementing processes within the domain. Domain events are suited to inform other bounded contexts about specific business-related events that have occurred.\n\n##### When to use Entity Changed Events vs Domain Events?\n - Only data replication, then Entity Changed Event will suffice their need.\n - Executing business logic in reaction to change in another system, then Domain Events should be preferred.\n\n### Event Structure\n\n- \"Fat\" events are preferred. This is to ensure that a consumer is able to action an event without having to make a sync cal to the producer service.\n##### CDC\nThe structure of the CDC events are fixed by the [[Debezium Connectors]]\n##### Entity Change Event\nEntity change event contain the state of the entity after the change. Optional event metadata, like `company_id` or `employee_id` (when applicable) should be included on the root level for both `changed` and `deleted` events.\n\n```\nmessage FoorBarEvent {\n  oneOf event {\n    FooBarChangedEvent = 1,\n    FooBarDeletedEvent = 2\n  }\n}\n\nmessage FooBarChangedEvent {\n  string id = 1;\n  // ... optional additional references like `company_id` or `employee_id`\n  FooBar entity = 2;\n}\n\nmessage FooBarDeletedEvent {\n  string id = 1;\n  // ... optional additional references like `company_id` or `employee_id`\n}\n\nmessage FooBar {\n  string id = 1;\n  string status = 2;\n  int64 duration = 3;\n}\n```\n\nEntity ID must be present in the key of the [[Kafka]] record in addition to the payload (to allow partitioning, ordering, and compaction). The event metadata described below should be encoded as [[Kafka]] record headers. Notes, that there is no `eventName` specified neither in the message payload, nor in the metadata. Parsing event payloads is make possible by using `oneOf` type definition.\n\nBecause the event log is only guaranteed to have the latest Entity Changed Event and not the full history of events. Not all entity change events are guaranteed to be processed by all consumers. Hence, having both the previous and current state of the entity inside the entity change event can cause issues when reprocessing the messages from a compacted log or restarting a consumer after a long pause.\nWhen using Kafka, we must provide a way to automatically remove records that were deleted from the event log. For this, a tombstone message with null values is required. Hence, consumer should be prepared to handle those messages and should not rely on the `entity deleted` messages in the long run.\n\n### Versioning\nFollow this guide when you want to update the ECEs message: https://developers.google.com/protocol-buffers/docs/proto3#updating\n","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Fault-tolerance":{"title":"Fault tolerance","content":"","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Fix-issues":{"title":"Fix issues","content":"https://stackoverflow.com/a/60694172","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Kafka":{"title":"Kafka","content":"","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Kafka-CLI":{"title":"Kafka CLI","content":"\n\nhttps://github.com/birdayz/kaf\n\n#### Create a new topic\n\n```\n$ kaf topic create kafka-in-actions\n \nâœ… Created topic!\n      Topic Name:            kafka-in-actions\n      Partitions:            -1\n      Replication Factor:    1\n      Cleanup Policy:        delete\n```\n\n#### List all topics\n\n```\n$ kaf topics\nNAME               PARTITIONS   REPLICAS\nkafka-in-actions   1            1\n```\n\n","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Kafka-in-Action-Book":{"title":"Kafka in Action Book","content":"\n### Introduction to Kafka\n\nKafka is leading the way to move from ETL - extract, transform and load and batch workflows to near-real-time data feeds.\n\n##### What is Kafka\nas a distributed streaming platform. It has three main capabilities:\n- Reading and writing records like a message queue\n- Storing records with fault-tolerant\n- Processing streams as they occur\n\n```mermaid\ngraph TD;\nA[Producer service]--\u003eB[Kafka cluster]--\u003eC[Consumer service 1];\nD[Producer service]--\u003eB;\nB--\u003eE[Consumer service 2]\n```\n\nData doesn't have to be limited to only a single destination. The producers and consumers are completely decoupled, allow each client to work independently.\n\nDelivery methods:\n- _At least-once semantics_ - A message is sent  as needed until it is acknowledged.\n- _At most-once semantics_ - A message is only sent once and not resent on failure\n- _Exactly-once semantics_ - A message is only seen once by the consumer of the message.\n\n##### At least once semantics\n![[notes/images/kafka-at-least-once.png]]\n\n##### At most once sematics\n\n![[notes/images/kafka-at-most-once.png]]\n\n##### Exactly once semantics\n\n![[notes/images/kafka-exactly-once.png]]\n\n- \"Dogfoods\" itself\nFor example, Kafka uses topics internally to manage consumer's offsets.\n\n##### Kafka is not the same as other message broker\n- The ability to replay messages by default\n- parallel processing of data\n- Kafka was designed to have multiple consumers.\n\n##### Kafka in real-world\n- XMPP (Extensible Messaging and Presence Protocol)\n- JMS - Java Message Service (Jakarta EE)\n- OASIS Advanced Message Queuing Protocol (AMQP)\n\nFeatures:\n- HA\nApache Flume - data replication\n- Log aggregation - the log files are sent as messages into Kafka, and then different applications have a single logical topic to consume that information.\n\n##### When Kafka might not be the right fit\n- You only need a once-monthly or even once-yearly summary of aggregate data.\n- Random lookup of data\n- exact ordering of messages \n- larger than 1MB message size\n\n\n### Getting to know Kafka\n\nKafka is a distributed system at heart, but it also possible to install and run it on a single host.\n\n\n##### Producing and consuming a message\n![[notes/images/kafka-message-structure.png]]\n\n##### What are brokers?\nBrokers can be thought of as the server side of Kafka.\n\n[[Kafka CLI]]","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Microservices":{"title":"Microservices","content":"[[Microservices Patterns]]","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Microservices-Patterns":{"title":"Microservices Patterns","content":"\n[[Two-phase commit]]\n\n[[SAGA]]\n\n[[Transactional Outbox]]\n\n[[Event Sourcing]]\n\n[[Database per Service]]","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Parallel-Pipeline":{"title":"Parallel Pipeline","content":"","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Pessimistic-lock":{"title":"Pessimistic lock","content":"","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/PostgreSQL":{"title":"PostgreSQL","content":"\nhttps://amplitude.engineering/how-a-single-postgresql-config-change-improved-slow-query-performance-by-50x-85593b8991b0","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Reading-list":{"title":"Reading list","content":"https://danluu.com/simple-architectures/","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Red-black-tree":{"title":"Red-black tree","content":"","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Replication":{"title":"Replication","content":"\n![[notes/images/leader-and-followers.png]]\n\n### Leader and followers\n- _leader_ ( #master or primary) - when clients want to write to database, they must send their requests to the leader, which first writes data to its local storage.\n- _followers_ (read replicas, #slaves, secondaries, hot standbys) - whenever the leader writes new data to its local storage, it also sends the data change to all of its followers as part of replication log or change stream.\n\n### Synchronous and Asynchronous Replication\n##### Synchronous\n![[notes/images/Synchronous-replication.png]]\n\nPros:\n- the #follower is guaranteed to have an up-to-date copy of data that is consistent with the #leader\n- if the #leader suddenly fails, we can be sure that the data is still available on the follower.\n\nCons:\n- if the synchronous #follower doesn't respond, the write cannot be processed.\n- the #leader must block all and wait until the synchronous replica is available again.\n- any one node outage would cause the whole system down. In practice, if you enable #synchronous replication on a database, it usually means that one of the followers is #synchronous, and the others are #asynchronous, it is called #semi-synchronous\n\n##### Asynchronous\n![[notes/images/Asynchronous.png]]\n- widely used even weakening durability\n\n### Setting up new Followers\n1. take a consistent snapshot of the leader's database at some point in time - if possible, without taking a lock on the entire database.\n2. Copy the snapshot to the follower node.\n3. The follower connects to the leader and requests all the data changes that have happened since the snapshot was taken.\n4. When the follower has processed the backlog of data changes since the snapshot. It can now continue to process data changes from the leader as they happen.\n\n### Handing node outages\n##### Follower failure: Catch-up recovery\nOn its local disk, each follower keeps a log of data changes it has received from the leader. If a follower crashes and is restarted, or if the network between the leader and followers is temporarily interrupted, the follower can recover quite easily:\n- from its log, it knows the last transaction that was processed before the fault occurred. Thus, the follower can connect to the leader and request all data changes that occurred during the time when the follower was disconnected.\n##### Leader failure: Failover\n","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/SAGA":{"title":"SAGA","content":"# SAGA\n![[notes/images/SAGA.png]]\n\n### Two ways of coordination SAGA\nThere are two ways of coordination sagas:\n- Choreography - each local transaction publish domain events that trigger local transactions in other services.\n- Orchestration - an orchestrator (object) tells the participants what local transactions to execute.\n\n### Resulting context\n##### Pros\n- It enables an application to maintain data consistency across multiple services without using distributed transaction.\n- The programming model is more complex. For example, a developer must design compensating that explicitly undo changes made earlier in a saga.\n\n##### Issues to address\n- In order to be reliable, a service must atomically update its database and publish a message/event. It cannot use the traditional mechanism of a distributed transaction that spans the database and the message broker. Instead, it must use one of the patterns list below:\n\t- [[Event Sourcing]]\n\t- [[Transactional Outbox]]\nA choreography-based saga can publish events using [[Aggregates]] and [[Domain Events]]\n- A client that initiates the saga, which an asynchronous flow, using a synchronous request (e.g. HTTP `POST /orders`) needs to be able to determine its outcome. There are different options, each with different trade-off:\n\t- The service sends back a response once the saga completes, e.g. once it receives `OrderApproved` or `OrderRejected` event.\n\t- The service sends back a response (e.g. containing the `orderId`) after initiating the saga, and the client periodically #polls (e.g. `GET /orders/{orderId}`) to determine the outcome\n\t- The service sends back a response (e.g. containing the `orderId`) after initiating the saga, and then sends an event (e.g. #websocket, #webhook, etc.) to the client once the saga completes.\n\n[[SAGA pattern with Orchestrator and Choreography]]\n\n### Resources\n- https://microservices.io/patterns/data/saga.html\n- https://www.youtube.com/watch?v=cpdL73GsM5c","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/SAGA-pattern-with-Orchestrator-and-Choreography":{"title":"SAGA pattern with Orchestrator and Choreography","content":"### Orchestrator: Command based\n\n#### Pros\n- Good for complex workflow / less coupling\n- Separation of concerns\n\n#### Cons \n- [[Single point of failure]]\n- [[Bottle neck]]\n\n![[notes/images/Orchestrator service.png]]\n### Choreography: Event based\n\u003e choÂ·reÂ·ogÂ·raÂ·phy (Definition: The sequence of steps and movements in dance or figure skating, especially in a ballet or other staged dance.)\n![[notes/images/Event base - Choreography.png]]\n\n#### Pros\n- No extra service / Simplicity\n- No [[Single point of failure]]\n- Loose coupling / [[Fault tolerance]]\n\n#### Cons\n- Difficult to maintain/understand\n- Risk of cyclic dependency\n\n### When to choose which?\n#### Business processing model\nIf you don't care about centralizing business process â†’ **Choreography**\nIf you do â†’ **Orchestration**\n\n#### Service coupling\n**Choreography** \u003e **Orchestrator**\n\n#### Transaction management\n","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/SSTable":{"title":"SSTable","content":"\n#database \n[[Red-black tree]]\n[[AVL Tree]]\n\nWhen a write comes in, add it to an in-memory balanced tree data structure ([[memtable]])\nWhen the [[memtable]] gets bigger than the thresh-hold, typically a few megabytes, write out it to disk as an [[SSTable]]\nThe new [[SSTable]] becomes the most recent segment of the database, while the [[SSTable]] is being written into disk, writes can continue to a new [[memtable]] instance. \n\nIn order to serve a read request, first try to find the key in the [[memtable]], the most recent on-disk segment, then in the next older segment, etc. \n\nFrom time to time, run a merging and compaction process in the background to combine segment files and to discard overwritten or deleted values. \n","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Service-Account-Token-Volume-Projection":{"title":"Service Account Token Volume Projection","content":"https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Service-to-Service-Authentication":{"title":"Service-to-Service Authentication","content":"https://medium.com/in-the-weeds/service-to-service-authentication-on-kubernetes-94dcb8216cdc\n\n![[notes/images/Pasted image 20221004112657.png]]\n\n[[Service Account Token Volume Projection]]","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Single-point-of-failure":{"title":"Single point of failure","content":"","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Spring-Framework":{"title":"Spring Framework","content":"\n# Spring framework\n\n![[notes/images/Spring's basic functional area.png]]\n\n### The Core Spring Container\n\n### Aspect-oriented programming (AOP)\n\n### Data Access/Integration\n\n### DI\nhttps://martinfowler.com/articles/injection.html#ConstructorVersusSetterInjection\n\n### POJO\n#pojo Plain Old Java Object\nhttps://www.geeksforgeeks.org/pojo-vs-java-beans/\n\n\n### Wiring beans using XML\n\n```java\npublic AccountService(AccountDao accountDao) {\n  this.accountDao = accountDao;\n}\n```\n\nIn Spring, you resolve the dependency like this:\n\n```xml\n\u003cbean id=\"accountService\" class=\"com.springinpractice.ch01.service.AccountService\"\u003e\n  \u003cconstructor-arg ref=\"accountDao\"/\u003e\n\u003c/bean\u003e\n```\n\n#PropertyPlaceholderConfigurer\n\n\n### Bean scopes\n- #singleton default scope for beans in Spring, \n- prototype\n- request\n- session\n- global session\n\n### JDBC\n\n","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Three-phase-commit":{"title":"Three phase commit","content":"","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Tools":{"title":"Tools","content":"Zero cost, no code checklist for pre-launch startups:\n- Typedream to build the landing page\n- Copy.ai to produce text for landing page\n- Canva to create designs\n- Figma to show mockups of your app in action\n- Loom or Vidyard to have a founder talk through the product mockups in videos\n- Pitch to create downloadable ebooks or presentations about product\n- Buffer to publish word of this new page on social media\n- tally.so to create forms to collect interested users\n- Airtable to check the early signup forms in one place\n- Mailchimp to manage mailing lists for interested early users\n- Slack to manage community of interested early users\n- Make to create tasks and workflows in backend\n- Notion to handle project management","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Transactional-Outbox":{"title":"Transactional Outbox","content":"#distributed-system #pattern \n### Transactional Outbox","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Two-phase-commit":{"title":"Two-phase commit","content":"#transaction #ACID\n- Atomicity\n- Consistency\n- Isolation\n- Durability\n\n![[notes/images/all-all-nothing.png.png]]\n\n### Monolithic\n```sql\nBEGIN TRANSACTION;\n\nINSERT INTO ....;\nUPDATE ...;\nDELETE ...;\n\nCOMMIT;\n```\n\n[transaction isolation](https://viblo.asia/p/014-postgresql-transaction-isolation-OeVKB67JKkW)\n\n### Distributed system\n- #distributed-system\n\n[[Dual writes]]\n\n![[notes/images/micro-service.png]]\n\n\u003e What happens when user was charged but order is not created?\n\n### Distributed  Transaction\n- [[notes/Two-phase commit]]\n- [[notes/Three phase commit]]]\n- [[notes/SAGA pattern with Orchestrator and Choreography]]\n- [[notes/Parallel Pipeline]]\n\n##### Crazy ideas\n- Single database\n- replicate/cluster database\n\t- #eventual-consistency \n\n### Two phase commit\n- Prepare phase\n- Commit phase\n\n![[notes/images/two-phase commit coordinator.png]]\n\n**Coordinator** can be an sub-module of #micro-service or separated #micro-service \n\n#### Prepare phase\n- **Payment Service**\n\t- BEGIN TRANSACTION\n\t- check the balance, if not `OK`, response `ERROR`\n\t- update blance\n\t- response `OK`\n- **Order Service**\n\t- BEGIN TRANSACTION\n\t\t- check quantity, if not enough, response `ERROR`\n\t\t- create order, update remaining quantity\n\t\t- response `OK`\n\n![[notes/images/prepare phase.png]]\n\nNow in each service, there is a local transaction is created and record is blocked, so overally, the global isolation is guaranteed.\n\nIf all services response with `OK`, the **Coordinator** will execute the next phase: Commit Phase. If not, the **Coordinator** will send a rollback request to all services\n\n_Coordinator has to wait all response from microservices before deciding the next action: rollback or commit, so a timeout is necessary._\n\n#### Commit phase\nAfter collecting all `OK` responses from microservices, Coordinator will send a request to commit all transactions.\n\n![[notes/images/Commit phase.png]]\n\nIf all local transaction are successfully committed, Coordinator could finish its work here.\n\n#### Drawback\n- **Latency**: Coordinator needs to wait replies from all microservices to decide what to do next. All transactions need to be a [[Pessimistic lock]]\n- **Coordinator** is a [[Single point of failure]], all transactions will be locked until the coordinator is back.\n- **Transaction dependency**: all local transactions will be dependent on each other, a transaction needs to wait until the last responses, `resource leak` might happen.\n- **Eventually Consistency**: there is a latency between microservices, so it isn't _really_ consistency.\n","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/Union-Find":{"title":"Union Find","content":"\n Given the below figure shows list of vertices and edges connected between them, how can quickly check if vertices[0] and vertices[3] are connected?\n \n![[notes/images/Pasted image 20221222000955.png]]\n\nWe can do so by using the #union-find data structure, it is also known as #disjoint-set data structure.\n\nThe primary use of #disjoint-set is to address the connectivity between the components of a network. The _network_ here can be a computer network or a social network. For instance, we can use #disjoint-set to determine if two people share a common ancestor.\n\n### Implementing Union Find\n\nA Union Find supports two basic method `find` and `union` methods.\n\n- `find(x: Int): Int` Allows determining which set that item `x` belongs to.\n- `union(x: Int, y: Int)` Merge two sets, where item `x` and item `y` belongs to.\n\n```kotlin\ninterface UnionFind {  \n    fun find(x: Int): Int  \n    fun union(x: Int, y: Int)  \n}\n```\n\n\n### Quick Find \n\n```kotlin\nclass QuickUnionFind(size: Int): UnionFind(size) {  \n    override fun find(x: Int): Int {  \n        return parent[x]  \n    }  \n  \n    override fun union(x: Int, y: Int) {  \n        val setX = find(x)  \n        val setY = find(y)  \n        if (setX != setY) {  \n            for (i in parent.indices) {  \n                if (parent[i] == setY) {  \n                    parent[i] = setX  \n                }  \n            }  \n        }  \n    }  \n}\n```\n\n##### Time Complexity\n- Constructor: **O(n)**\n- Find: **O(1)**\n- Union: **O(n)**\n\n\n### Quick Union\n\n```kotlin\nclass QuickUnion(size: Int) : UnionFind {  \n    private val parent: IntArray = IntArray(size)  \n  \n    init {  \n        for (i in 0 until size) {  \n            parent[i] = i  \n        }  \n    }  \n  \n    override fun find(x: Int): Int {  \n        if (parent[x] == x) return x  \n  \n        return find(parent[x])  \n    }  \n  \n    override fun union(x: Int, y: Int) {  \n        val setX = find(x)  \n        val setY = find(y)  \n  \n        if (setX != setY) {  \n            parent[setX] = setY  \n        }  \n    }  \n}\n```\n\n##### Time Complexity\n- Constructor: **O(n)**\n- find: **O(n)**\n- union: **O(n)**\n\n##### Why Quick Union is more efficient than Quick Union?\nThe `union` operation consists of two `find` operations which (only in the worst case) will take **O(N)** time, and two constant time operations, including the equality check and updating the array value at the given index. Therefore, the `union` operation also cost **O(N)** in the worst case.\n\n\n### Union By Rank\n\n```kotlin\nclass UnionByRank(size: Int) : UnionFind {  \n    private val parent = IntArray(size)  \n    private val rank = IntArray(size)  \n  \n    init {  \n        for (i in 0 until size) {  \n            parent[i] = i  \n        }  \n    }  \n  \n    override fun find(x: Int): Int {  \n\t    var currentX = x;  \n\t    while (currentX != parent[currentX]) {  \n\t        currentX = parent[currentX]  \n\t    }  \n\t    return currentX  \n\t}\n  \n    override fun union(x: Int, y: Int) {  \n        val setX = find(x)  \n        val setY = find(y)  \n  \n        if (rank[setX] \u003e rank[setY]) {  \n            parent[setY] = parent[setX]  \n        } else if (rank[setX] \u003c rank[setY]) {  \n            parent[setX] = parent[setY]  \n        } else {  \n            parent[setY] = parent[setX]  \n            rank[setX]++  \n        }  \n    }  \n}\n```\n\n##### Time Complexity\n\n- Constructor: **O(n)**\n- find: **O(log n)**\n- union: **O(log n)**\n\n### The Path Compression Optimization\n\n","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/gRPC":{"title":"gRPC","content":"\nhttps://developers.google.com/protocol-buffers/docs/proto3#updating","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/gRPC-CLI-cheatsheet":{"title":"gRPC CLI cheatsheet","content":"\n### ls\n```\ngrpc_cli ls localhost:9090\n```\n","lastmodified":"2022-12-31T09:26:54.586210686Z","tags":null},"/notes/memtable":{"title":"memtable","content":"","lastmodified":"2022-12-31T09:26:54.602211132Z","tags":null},"/notes/read-after-write":{"title":"read-after-write","content":"\nRead-after-write consistency isÂ **the ability to view changes (read data) right after making those changes (write data)**. For example, if you have a user profile and you change your bio on the profile, you should see the updated bio if you refresh the page. There should be no delay during which the old bio shows up.","lastmodified":"2022-12-31T09:26:54.602211132Z","tags":null}}
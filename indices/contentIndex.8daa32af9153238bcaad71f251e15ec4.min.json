{"/":{"title":"🪴 Nam's Notes.","content":"\nHi, I'm Nam. I do stuffs with computer and sometimes write about it here. I'm from Danang, VN. Currently I ship code at Personio.\n\nIn addition to reading books, playing Dota2, I'm deeply passionate about distributed system and [leetcode](https://leetcode.com/) everyday.\n\nThis is my daily notes, I'm using [Obsidian](https://obsidian.md/) as my knowledge base.\n\n[All my notes](/notes)\n","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/Network-timing-breakdown-phases.png":{"title":"","content":"","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/AVL-Tree":{"title":"AVL Tree","content":"","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Aggregates":{"title":"Aggregates","content":"\n#distributed-system #pattern ","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Bottle-neck":{"title":"Bottle neck","content":"","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Code-review":{"title":"Code review","content":"![[notes/images/CFFDADEE-EB19-496B-B9A2-BF726A97E05E.jpeg]]","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Database-per-Service":{"title":"Database per Service","content":"#distributed-system #pattern ","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Debezium-Connectors":{"title":"Debezium Connectors","content":"","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Design-data-intensive-application":{"title":"Design data intensive application","content":"\n#book #database\n\n[[SSTable]]\n\n[[Replication]]","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Distributed-Systems":{"title":"Distributed Systems","content":"\n[[read-after-write]]","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Domain-Events":{"title":"Domain Events","content":"#distributed-system #pattern ","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Dual-writes":{"title":"Dual writes","content":"\n#dual-writes\n### Resources\n- https://developers.redhat.com/articles/2021/07/30/avoiding-dual-writes-event-driven-applications#","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/ECEs-Entity-Changed-Events":{"title":"ECE","content":"\n#### Production and consumption\n\nIn order to produce ECEs, one need to:\n- Create an ECE topic following the naming convention\n- In the producer service, leverage the ebdr-outbox library\n- Create a connector\nIn order to consume ECEs, one need to:\n- Give proper permission to the AWS IAM role associated with the service which is going to consume ECEs\n- in consumer service, leverage ebdr-consumer library","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Event-Sourcing":{"title":"Event Sourcing","content":"\n#distributed-system #pattern ","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Events":{"title":"Events","content":"\n### Event Types\n\n#### Change Data Capture Events (CDC)\n- refers to the process of capturing changes made to a database\n- Row level changes or schema level changes\n- CDC Events **MUST NOT** be consumed by consumers (µS) outside the Bounded Context which owns the data\n- For example: Company data is owned and served by Monolith. Extracting all logic into a separate µS requires data to be migrated as well, since every service maintains its own datastore. Having CDC event streaming in place would allow the µS to migrate the changes to its datastore.\n- Visibility: CDC events are private messages, and hence the events must be produced to/consumed from private topics. Kafka allows configuring ACL for restricted accesses.\n#### Entity Changed Events (ECE)\n[[ECEs - Entity Changed Events]]\n- represent a data structure with the intent of presenting changes in a entity. This at first might sound similar to CDC, but conceptually they're different. ECE  Events define a public contract, which can be used by Foreign Bounded Contexts. They're used primarily for **Data Replication** between different systems. For the consumers that listen to ECEs, the guarantee is that they will receive the latest state of an entity, but might not receive all the intermediates states, due to #compaction.\n\n##### How is different from CDC?\n- CDC could be the basis of Entity Changed Events, there are scenarios where the produced may directly produce such an event without need for CDC.\n- CDC events are granular as they can expose Table/Schema/Row Level changes.\n- ECEs can communicate changes on Domain Entity, i.e. `Company` could be a domain model and `CompanyChangedEvent` will present changes on that model. The important point here is that the persistence model could span several tables.\n\n#### Domain Events\nDomain Events are described as something that happened in the domain. Just like Entity Changed Events, domain events also define a public contract. Such events typically occur regardless of whether or to what extent the domain is implemented in software development. They are also independent of technologies.\n- An employee is hired\n- An employee is fired\n- An candidate rejected an offer.\n\nDomain events are relevant both within a bounded context and across bounded context for implementing processes within the domain. Domain events are suited to inform other bounded contexts about specific business-related events that have occurred.\n\n##### When to use Entity Changed Events vs Domain Events?\n - Only data replication, then Entity Changed Event will suffice their need.\n - Executing business logic in reaction to change in another system, then Domain Events should be preferred.\n\n### Event Structure\n\n- \"Fat\" events are preferred. This is to ensure that a consumer is able to action an event without having to make a sync cal to the producer service.\n##### CDC\nThe structure of the CDC events are fixed by the [[Debezium Connectors]]\n##### Entity Change Event\nEntity change event contain the state of the entity after the change. Optional event metadata, like `company_id` or `employee_id` (when applicable) should be included on the root level for both `changed` and `deleted` events.\n\n```\nmessage FoorBarEvent {\n  oneOf event {\n    FooBarChangedEvent = 1,\n    FooBarDeletedEvent = 2\n  }\n}\n\nmessage FooBarChangedEvent {\n  string id = 1;\n  // ... optional additional references like `company_id` or `employee_id`\n  FooBar entity = 2;\n}\n\nmessage FooBarDeletedEvent {\n  string id = 1;\n  // ... optional additional references like `company_id` or `employee_id`\n}\n\nmessage FooBar {\n  string id = 1;\n  string status = 2;\n  int64 duration = 3;\n}\n```\n\nEntity ID must be present in the key of the [[Kafka]] record in addition to the payload (to allow partitioning, ordering, and compaction). The event metadata described below should be encoded as [[Kafka]] record headers. Notes, that there is no `eventName` specified neither in the message payload, nor in the metadata. Parsing event payloads is make possible by using `oneOf` type definition.\n\nBecause the event log is only guaranteed to have the latest Entity Changed Event and not the full history of events. Not all entity change events are guaranteed to be processed by all consumers. Hence, having both the previous and current state of the entity inside the entity change event can cause issues when reprocessing the messages from a compacted log or restarting a consumer after a long pause.\nWhen using Kafka, we must provide a way to automatically remove records that were deleted from the event log. For this, a tombstone message with null values is required. Hence, consumer should be prepared to handle those messages and should not rely on the `entity deleted` messages in the long run.\n\n### Versioning\nFollow this guide when you want to update the ECEs message: https://developers.google.com/protocol-buffers/docs/proto3#updating\n","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Fault-tolerance":{"title":"Fault tolerance","content":"","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Fix-issues":{"title":"Fix issues","content":"https://stackoverflow.com/a/60694172","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/How-does-HTTPS-work":{"title":"How does HTTPS work?","content":"\n[Youtube - How does HTTPS work?](https://www.youtube.com/watch?t=371\u0026v=67kItGjvRs0\u0026feature=youtu.be)","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Kafka":{"title":"Kafka","content":"","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Kafka-CLI":{"title":"Kafka CLI","content":"\n\nhttps://github.com/birdayz/kaf\n\n#### Create a new topic\n\n```\n$ kaf topic create kafka-in-actions\n \n✅ Created topic!\n      Topic Name:            kafka-in-actions\n      Partitions:            -1\n      Replication Factor:    1\n      Cleanup Policy:        delete\n```\n\n#### List all topics\n\n```\n$ kaf topics\nNAME               PARTITIONS   REPLICAS\nkafka-in-actions   1            1\n```\n\n","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Kafka-in-Action-Book":{"title":"Kafka in Action Book","content":"\n### Introduction to Kafka\n\nKafka is leading the way to move from ETL - extract, transform and load and batch workflows to near-real-time data feeds.\n\n##### What is Kafka\nas a distributed streaming platform. It has three main capabilities:\n- Reading and writing records like a message queue\n- Storing records with fault-tolerant\n- Processing streams as they occur\n\n```mermaid\ngraph TD;\nA[Producer service]--\u003eB[Kafka cluster]--\u003eC[Consumer service 1];\nD[Producer service]--\u003eB;\nB--\u003eE[Consumer service 2]\n```\n\nData doesn't have to be limited to only a single destination. The producers and consumers are completely decoupled, allow each client to work independently.\n\nDelivery methods:\n- _At least-once semantics_ - A message is sent  as needed until it is acknowledged.\n- _At most-once semantics_ - A message is only sent once and not resent on failure\n- _Exactly-once semantics_ - A message is only seen once by the consumer of the message.\n\n##### At least once semantics\n![[notes/images/kafka-at-least-once.png]]\n\n##### At most once sematics\n\n![[notes/images/kafka-at-most-once.png]]\n\n##### Exactly once semantics\n\n![[notes/images/kafka-exactly-once.png]]\n\n- \"Dogfoods\" itself\nFor example, Kafka uses topics internally to manage consumer's offsets.\n\n##### Kafka is not the same as other message broker\n- The ability to replay messages by default\n- parallel processing of data\n- Kafka was designed to have multiple consumers.\n\n##### Kafka in real-world\n- XMPP (Extensible Messaging and Presence Protocol)\n- JMS - Java Message Service (Jakarta EE)\n- OASIS Advanced Message Queuing Protocol (AMQP)\n\nFeatures:\n- HA\nApache Flume - data replication\n- Log aggregation - the log files are sent as messages into Kafka, and then different applications have a single logical topic to consume that information.\n\n##### When Kafka might not be the right fit\n- You only need a once-monthly or even once-yearly summary of aggregate data.\n- Random lookup of data\n- exact ordering of messages \n- larger than 1MB message size\n\n\n### Getting to know Kafka\n\nKafka is a distributed system at heart, but it also possible to install and run it on a single host.\n\n\n##### Producing and consuming a message\n![[notes/images/kafka-message-structure.png]]\n\n##### What are brokers?\nBrokers can be thought of as the server side of Kafka.\n\n[[Kafka CLI]]","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Microservices":{"title":"Microservices","content":"[[Microservices Patterns]]","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Microservices-Patterns":{"title":"Microservices Patterns","content":"\n[[Two-phase commit]]\n\n[[SAGA]]\n\n[[Transactional Outbox]]\n\n[[Event Sourcing]]\n\n[[Database per Service]]","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Microservices-learning-resources":{"title":"Microservices learning resources","content":"\n- https://samnewman.io/talks/principles-of-microservices/\n\n- https://www.cloudbees.com/blog/5-ways-not-mess-microservices-production\n\n- https://martinfowler.com/bliki/IntegrationDatabase.html\n\n- https://samnewman.io/books/building_microservices/\n\n- https://martinfowler.com/bliki/StranglerFigApplication.html","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Network-timning-break-down-phases-explained":{"title":"Network timning break down phases explained","content":"\n![[notes/images/Pasted image 20221231172352.png]]\nHere's more information about each of the phases you may see in the Timing tab:\n- **Queuing**: The browser queues requests when:\n\t- There a higher priority requests\n\t- There are already six TCP connections open for this origin, which is the limit. Applies to HTTP/1.0 and HTTP/1.1 only.\n\t- The browser is briefly allocating space in the disk cache\n\n- **Stalled**: The request could be stalled for any of the reasons described in the **Queueing**.\n\n- **DNS Lookup**: The browser is resolving the request's IP address.\n\n- **Initial Connection**: The browser is establishing a connection, including TPC handshakes/retries and negotiating an SSL.\n\n- **Proxy negotiation**: The browser is negotiating the request with proxy server.\n\n- **Request sent**:  The request is being sent\n\n- **ServiceWorker Preparation**: The browser is starting up t he service worker.\n\n- **Request to ServiceWorker**: The request is being sent to the service worker.\n\n- **Waiting (TTFB)**: The browser is waiting for the first byte of a response. TTFB stands for Time for First Byte. This timing includes 1 round trip of latency and the time the server took to prepare the response.\n\n- **Content Download**: The browser is receiving the response, either directly from the network or from a service worker. This value is the total amount of time spent reading the response body. Larger than expected values could indicate a slow network, or that the browser is busy performing other work which delays the response from being read.\n\n- **Receiving Push**: The browser is receiving data for this response via HTTP/2 Server Push.\n\n- **Reading Push**: The browser is reading the local data previously received.","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Parallel-Pipeline":{"title":"Parallel Pipeline","content":"","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Pessimistic-lock":{"title":"Pessimistic lock","content":"","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/PostgreSQL":{"title":"PostgreSQL","content":"\nhttps://amplitude.engineering/how-a-single-postgresql-config-change-improved-slow-query-performance-by-50x-85593b8991b0","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/PostgreSQL-autovacuum-launcher-process":{"title":"PostgreSQL autovacuum launcher process","content":"\n\u003e “Assume that we delete a few records from a table. PostgreSQL does not immediately remove the deleted tuples from the data files. These are marked as deleted. Similarly, when a record is updated, it's roughly equivalent to one delete and one insert. The previous version of the record continues to be in the data file. Each update of a database row generates a new version of the row. The reason is simple: there can be active transactions, which want to see the data as it was before. As a result of this activity, there will be a lot of unusable space in the data files. After some time, these dead records become irrelevant as there are no transactions still around to see the old data. However, as the space is not marked as reusable, inserts and updates (which result in inserts) happen in other pages in the data file.”\n\n\u003e“The vacuum process marks space used by previously deleted (or updated) records as being available for reuse within the table. There is a vacuum command to do this manually. Vacuum does not lock the table.”\n\nExcerpt From\nPostgreSQL for Data Architects\nThis material may be protected by copyright.","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/ReactJS-rendering":{"title":"ReactJS rendering","content":"\n- https://twitter.com/sophiebits/status/1228942768543686656\n\n\u003e That React Component Right Under Your Context Provider Should Probably Use `React.memo`\n\n- https://github.com/facebook/react/issues/15156\n\n- https://tuhuynh.com/posts/react-rendering/\n\n-   [https://blog.isquaredsoftware.com/2020/05/blogged-answers-a-mostly-complete-guide-to-react-rendering-behavior/](https://blog.isquaredsoftware.com/2020/05/blogged-answers-a-mostly-complete-guide-to-react-rendering-behavior/)\n\n-   [https://reactjs.org/docs/reconciliation.html#elements-of-different-types](https://reactjs.org/docs/reconciliation.html#elements-of-different-types)\n\n- https://reactjs.org/docs/state-and-lifecycle.html#state-updates-may-be-asynchronous\n\n-   [https://github.com/vuejs/rfcs/issues/89](https://github.com/vuejs/rfcs/issues/89)","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Reading-list":{"title":"Reading list","content":"https://danluu.com/simple-architectures/","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Red-black-tree":{"title":"Red-black tree","content":"","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Replication":{"title":"Replication","content":"\n![[notes/images/leader-and-followers.png]]\n\n### Leader and followers\n- _leader_ ( #master or primary) - when clients want to write to database, they must send their requests to the leader, which first writes data to its local storage.\n- _followers_ (read replicas, #slaves, secondaries, hot standbys) - whenever the leader writes new data to its local storage, it also sends the data change to all of its followers as part of replication log or change stream.\n\n### Synchronous and Asynchronous Replication\n##### Synchronous\n![[notes/images/Synchronous-replication.png]]\n\nPros:\n- the #follower is guaranteed to have an up-to-date copy of data that is consistent with the #leader\n- if the #leader suddenly fails, we can be sure that the data is still available on the follower.\n\nCons:\n- if the synchronous #follower doesn't respond, the write cannot be processed.\n- the #leader must block all and wait until the synchronous replica is available again.\n- any one node outage would cause the whole system down. In practice, if you enable #synchronous replication on a database, it usually means that one of the followers is #synchronous, and the others are #asynchronous, it is called #semi-synchronous\n\n##### Asynchronous\n![[notes/images/Asynchronous.png]]\n- widely used even weakening durability\n\n### Setting up new Followers\n1. take a consistent snapshot of the leader's database at some point in time - if possible, without taking a lock on the entire database.\n2. Copy the snapshot to the follower node.\n3. The follower connects to the leader and requests all the data changes that have happened since the snapshot was taken.\n4. When the follower has processed the backlog of data changes since the snapshot. It can now continue to process data changes from the leader as they happen.\n\n### Handing node outages\n##### Follower failure: Catch-up recovery\nOn its local disk, each follower keeps a log of data changes it has received from the leader. If a follower crashes and is restarted, or if the network between the leader and followers is temporarily interrupted, the follower can recover quite easily:\n- from its log, it knows the last transaction that was processed before the fault occurred. Thus, the follower can connect to the leader and request all data changes that occurred during the time when the follower was disconnected.\n##### Leader failure: Failover\n","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/SAGA":{"title":"SAGA","content":"# SAGA\n![[notes/images/SAGA.png]]\n\n### Two ways of coordination SAGA\nThere are two ways of coordination sagas:\n- Choreography - each local transaction publish domain events that trigger local transactions in other services.\n- Orchestration - an orchestrator (object) tells the participants what local transactions to execute.\n\n### Resulting context\n##### Pros\n- It enables an application to maintain data consistency across multiple services without using distributed transaction.\n- The programming model is more complex. For example, a developer must design compensating that explicitly undo changes made earlier in a saga.\n\n##### Issues to address\n- In order to be reliable, a service must atomically update its database and publish a message/event. It cannot use the traditional mechanism of a distributed transaction that spans the database and the message broker. Instead, it must use one of the patterns list below:\n\t- [[Event Sourcing]]\n\t- [[Transactional Outbox]]\nA choreography-based saga can publish events using [[Aggregates]] and [[Domain Events]]\n- A client that initiates the saga, which an asynchronous flow, using a synchronous request (e.g. HTTP `POST /orders`) needs to be able to determine its outcome. There are different options, each with different trade-off:\n\t- The service sends back a response once the saga completes, e.g. once it receives `OrderApproved` or `OrderRejected` event.\n\t- The service sends back a response (e.g. containing the `orderId`) after initiating the saga, and the client periodically #polls (e.g. `GET /orders/{orderId}`) to determine the outcome\n\t- The service sends back a response (e.g. containing the `orderId`) after initiating the saga, and then sends an event (e.g. #websocket, #webhook, etc.) to the client once the saga completes.\n\n[[SAGA pattern with Orchestrator and Choreography]]\n\n### Resources\n- https://microservices.io/patterns/data/saga.html\n- https://www.youtube.com/watch?v=cpdL73GsM5c","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/SAGA-pattern-with-Orchestrator-and-Choreography":{"title":"SAGA pattern with Orchestrator and Choreography","content":"### Orchestrator: Command based\n\n#### Pros\n- Good for complex workflow / less coupling\n- Separation of concerns\n\n#### Cons \n- [[Single point of failure]]\n- [[Bottle neck]]\n\n![[notes/images/Orchestrator service.png]]\n### Choreography: Event based\n\u003e cho·re·og·ra·phy (Definition: The sequence of steps and movements in dance or figure skating, especially in a ballet or other staged dance.)\n![[notes/images/Event base - Choreography.png]]\n\n#### Pros\n- No extra service / Simplicity\n- No [[Single point of failure]]\n- Loose coupling / [[Fault tolerance]]\n\n#### Cons\n- Difficult to maintain/understand\n- Risk of cyclic dependency\n\n### When to choose which?\n#### Business processing model\nIf you don't care about centralizing business process → **Choreography**\nIf you do → **Orchestration**\n\n#### Service coupling\n**Choreography** \u003e **Orchestrator**\n\n#### Transaction management\n","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/SSTable":{"title":"SSTable","content":"\n#database \n[[Red-black tree]]\n[[AVL Tree]]\n\nWhen a write comes in, add it to an in-memory balanced tree data structure ([[memtable]])\nWhen the [[memtable]] gets bigger than the thresh-hold, typically a few megabytes, write out it to disk as an [[SSTable]]\nThe new [[SSTable]] becomes the most recent segment of the database, while the [[SSTable]] is being written into disk, writes can continue to a new [[memtable]] instance. \n\nIn order to serve a read request, first try to find the key in the [[memtable]], the most recent on-disk segment, then in the next older segment, etc. \n\nFrom time to time, run a merging and compaction process in the background to combine segment files and to discard overwritten or deleted values. \n","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Service-Account-Token-Volume-Projection":{"title":"Service Account Token Volume Projection","content":"https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Service-to-Service-Authentication":{"title":"Service-to-Service Authentication","content":"https://medium.com/in-the-weeds/service-to-service-authentication-on-kubernetes-94dcb8216cdc\n\n![[notes/images/Pasted image 20221004112657.png]]\n\n[[Service Account Token Volume Projection]]","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Setting-up-TailwindCSS-in-a-Parcel-project":{"title":"Setting up TailwindCSS in a Parcel project","content":"\n#### Initialize the project\n\n```\nmkdir tailwindcss-parcel-setup-example\ntailwindcss-parcel-setup-example\nyarn add -D parcel\nmkdir src\ntouch src/index.html\n```\n\n#### Install TailwindCSS\n```\nyarn add -D tailwindcss postcss\nnpx tailwindcss init\n```\n\n\n#### Configure PostCSS\n```json\n{\n\t\"plugins\": {\n\t\t\"tailwindcss\": {}\n\t}\n}\n```\n\n#### Configure template paths\n\n```js\nmodule.exports = {\n\tcontent: [\n\t\t\"./src/**/*.{html,js,ts,jsx,tsx}\",\n\t],\n\ttheme: {\n\t\textend: {}\n\t},\n\tplugins: [],\n}\n```\n\nRead more about `content` option [here](https://tailwindcss.com/docs/content-configuration).\n\n#### Add the tailwindcss directives to your CSS\n```css\n@tailwind base;\n@tailwind components;\n@tailwind utilities;\n```\n\n#### Start your build process\n\n```\nnpx parcel src/index.html\n```\n\n#### Start using Tailwind in your project\n\n```html\n\u003c!doctype html\u003e\n\u003chtml\u003e\n\u003chead\u003e\n  \u003cmeta charset=\"UTF-8\"\u003e\n  \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e\n  \u003clink href=\"./index.css\" rel=\"stylesheet\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n  \u003ch1 class=\"text-3xl font-bold underline\"\u003e\n    Hello world!\n  \u003c/h1\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n```","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Single-point-of-failure":{"title":"Single point of failure","content":"","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Spring-Framework":{"title":"Spring Framework","content":"\n# Spring framework\n\n![[notes/images/Spring's basic functional area.png]]\n\n### The Core Spring Container\n\n### Aspect-oriented programming (AOP)\n\n### Data Access/Integration\n\n### DI\nhttps://martinfowler.com/articles/injection.html#ConstructorVersusSetterInjection\n\n### POJO\n#pojo Plain Old Java Object\nhttps://www.geeksforgeeks.org/pojo-vs-java-beans/\n\n\n### Wiring beans using XML\n\n```java\npublic AccountService(AccountDao accountDao) {\n  this.accountDao = accountDao;\n}\n```\n\nIn Spring, you resolve the dependency like this:\n\n```xml\n\u003cbean id=\"accountService\" class=\"com.springinpractice.ch01.service.AccountService\"\u003e\n  \u003cconstructor-arg ref=\"accountDao\"/\u003e\n\u003c/bean\u003e\n```\n\n#PropertyPlaceholderConfigurer\n\n\n### Bean scopes\n- #singleton default scope for beans in Spring, \n- prototype\n- request\n- session\n- global session\n\n### JDBC\n\n","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Three-phase-commit":{"title":"Three phase commit","content":"","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Tools":{"title":"Tools","content":"Zero cost, no code checklist for pre-launch startups:\n- Typedream to build the landing page\n- Copy.ai to produce text for landing page\n- Canva to create designs\n- Figma to show mockups of your app in action\n- Loom or Vidyard to have a founder talk through the product mockups in videos\n- Pitch to create downloadable ebooks or presentations about product\n- Buffer to publish word of this new page on social media\n- tally.so to create forms to collect interested users\n- Airtable to check the early signup forms in one place\n- Mailchimp to manage mailing lists for interested early users\n- Slack to manage community of interested early users\n- Make to create tasks and workflows in backend\n- Notion to handle project management","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Transactional-Outbox":{"title":"Transactional Outbox","content":"#distributed-system #pattern \n### Transactional Outbox","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Two-phase-commit":{"title":"Two-phase commit","content":"#transaction #ACID\n- Atomicity\n- Consistency\n- Isolation\n- Durability\n\n![[notes/images/all-all-nothing.png.png]]\n\n### Monolithic\n```sql\nBEGIN TRANSACTION;\n\nINSERT INTO ....;\nUPDATE ...;\nDELETE ...;\n\nCOMMIT;\n```\n\n[transaction isolation](https://viblo.asia/p/014-postgresql-transaction-isolation-OeVKB67JKkW)\n\n### Distributed system\n- #distributed-system\n\n[[Dual writes]]\n\n![[notes/images/micro-service.png]]\n\n\u003e What happens when user was charged but order is not created?\n\n### Distributed  Transaction\n- [[Two-phase commit]]\n- [[Three phase commit]]\n- [[SAGA pattern with Orchestrator and Choreography]]\n- [[Parallel Pipeline]]\n\n##### Crazy ideas\n- Single database\n- replicate/cluster database\n\t- #eventual-consistency \n\n### Two phase commit\n- Prepare phase\n- Commit phase\n\n![[notes/images/two-phase commit coordinator.png]]\n\n**Coordinator** can be an sub-module of #micro-service or separated #micro-service \n\n#### Prepare phase\n- **Payment Service**\n\t- BEGIN TRANSACTION\n\t- check the balance, if not `OK`, response `ERROR`\n\t- update blance\n\t- response `OK`\n- **Order Service**\n\t- BEGIN TRANSACTION\n\t\t- check quantity, if not enough, response `ERROR`\n\t\t- create order, update remaining quantity\n\t\t- response `OK`\n\n![[notes/images/prepare phase.png]]\n\nNow in each service, there is a local transaction is created and record is blocked, so overally, the global isolation is guaranteed.\n\nIf all services response with `OK`, the **Coordinator** will execute the next phase: Commit Phase. If not, the **Coordinator** will send a rollback request to all services\n\n_Coordinator has to wait all response from microservices before deciding the next action: rollback or commit, so a timeout is necessary._\n\n#### Commit phase\nAfter collecting all `OK` responses from microservices, Coordinator will send a request to commit all transactions.\n\n![[notes/images/Commit phase.png]]\n\nIf all local transaction are successfully committed, Coordinator could finish its work here.\n\n#### Drawback\n- **Latency**: Coordinator needs to wait replies from all microservices to decide what to do next. All transactions need to be a [[Pessimistic lock]]\n- **Coordinator** is a [[Single point of failure]], all transactions will be locked until the coordinator is back.\n- **Transaction dependency**: all local transactions will be dependent on each other, a transaction needs to wait until the last responses, `resource leak` might happen.\n- **Eventually Consistency**: there is a latency between microservices, so it isn't _really_ consistency.\n","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Union-Find":{"title":"Union Find","content":"\n Given the below figure shows list of vertices and edges connected between them, how can quickly check if vertices[0] and vertices[3] are connected?\n \n![[notes/images/Pasted image 20221222000955.png]]\n\nWe can do so by using the #union-find data structure, it is also known as #disjoint-set data structure.\n\nThe primary use of #disjoint-set is to address the connectivity between the components of a network. The _network_ here can be a computer network or a social network. For instance, we can use #disjoint-set to determine if two people share a common ancestor.\n\n### Implementing Union Find\n\nA Union Find supports two basic method `find` and `union` methods.\n\n- `find(x: Int): Int` Allows determining which set that item `x` belongs to.\n- `union(x: Int, y: Int)` Merge two sets, where item `x` and item `y` belongs to.\n\n```kotlin\ninterface UnionFind {  \n    fun find(x: Int): Int  \n    fun union(x: Int, y: Int)  \n}\n```\n\n\n### Quick Find \n\n```kotlin\nclass QuickUnionFind(size: Int): UnionFind(size) {  \n    override fun find(x: Int): Int {  \n        return parent[x]  \n    }  \n  \n    override fun union(x: Int, y: Int) {  \n        val setX = find(x)  \n        val setY = find(y)  \n        if (setX != setY) {  \n            for (i in parent.indices) {  \n                if (parent[i] == setY) {  \n                    parent[i] = setX  \n                }  \n            }  \n        }  \n    }  \n}\n```\n\n##### Time Complexity\n- Constructor: **O(n)**\n- Find: **O(1)**\n- Union: **O(n)**\n\n\n### Quick Union\n\n```kotlin\nclass QuickUnion(size: Int) : UnionFind {  \n    private val parent: IntArray = IntArray(size)  \n  \n    init {  \n        for (i in 0 until size) {  \n            parent[i] = i  \n        }  \n    }  \n  \n    override fun find(x: Int): Int {  \n        if (parent[x] == x) return x  \n  \n        return find(parent[x])  \n    }  \n  \n    override fun union(x: Int, y: Int) {  \n        val setX = find(x)  \n        val setY = find(y)  \n  \n        if (setX != setY) {  \n            parent[setX] = setY  \n        }  \n    }  \n}\n```\n\n##### Time Complexity\n- Constructor: **O(n)**\n- find: **O(n)**\n- union: **O(n)**\n\n##### Why Quick Union is more efficient than Quick Union?\nThe `union` operation consists of two `find` operations which (only in the worst case) will take **O(N)** time, and two constant time operations, including the equality check and updating the array value at the given index. Therefore, the `union` operation also cost **O(N)** in the worst case.\n\n\n### Union By Rank\n\n```kotlin\nclass UnionByRank(size: Int) : UnionFind {  \n    private val parent = IntArray(size)  \n    private val rank = IntArray(size)  \n  \n    init {  \n        for (i in 0 until size) {  \n            parent[i] = i  \n        }  \n    }  \n  \n    override fun find(x: Int): Int {  \n\t    var currentX = x;  \n\t    while (currentX != parent[currentX]) {  \n\t        currentX = parent[currentX]  \n\t    }  \n\t    return currentX  \n\t}\n  \n    override fun union(x: Int, y: Int) {  \n        val setX = find(x)  \n        val setY = find(y)  \n  \n        if (rank[setX] \u003e rank[setY]) {  \n            parent[setY] = parent[setX]  \n        } else if (rank[setX] \u003c rank[setY]) {  \n            parent[setX] = parent[setY]  \n        } else {  \n            parent[setY] = parent[setX]  \n            rank[setX]++  \n        }  \n    }  \n}\n```\n\n##### Time Complexity\n\n- Constructor: **O(n)**\n- find: **O(log n)**\n- union: **O(log n)**\n\n### The Path Compression Optimization\n\n","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Unix-Process-notes":{"title":"Unix Process notes","content":"\n### Process ID\nMỗi process đều có một unique ID, nó giống như số chứng minh nhân dân của mình vậy.\nThử chạy command sau ở `irb` (interactive ruby), ta có process đang chạy `irb` có ID là `96160`.\n\n```ruby\nputs Process.pid\n =\u003e 96160\n```\n\nMở một terminal khác rồi chạy command sau, ta sẽ lấy được một số thông tin cơ bản về process đó thông qua ID.\n```console\nps -p \u003cprocess-id\u003e\n  PID TTY           TIME CMD\n96160 ttys002    0:00.45 irb\n```\n\n### Parent process\nMỗi process chạy OS đều có process cha, nó có thể lấy process ID của process cha bằng `ppid`\n\n```ruby\nProcess.ppid\n =\u003e 24569\n```\n\nThử đoán xem process có ID `24569` là process nào?\n\n```console\nps -p 24569\n  PID TTY           TIME CMD\n24569 ttys002    0:04.95 -zsh\n```\n\nVì mình chạy `irb` thông qua `zsh` nên process cha của `irb` sẽ là `zsh`.\n\n### File descriptor\n\n\u003e in the land of Unix ‘everything is a file’\n\nGiống như mỗi process đều có một unique ID, bất cứ lúc nào chúng ta mở một resource (files, sockets, devices,...) trong một process, thì resource đó cũng được đánh số thứ tự `fileno`\n`fileno` sẽ luôn là số nhỏ nhất mà chưa được sử dụng\n\nRun command sau ở `irb`.\n\n```ruby\npasswd = File.open('/etc/passwd')\nputs passwd.fileno\n =\u003e 3\n\nhosts = File.open('/etc/hosts')\nputs hosts.fileno\n =\u003e 4\n\n# Khi close file, thì file descriptor cũng sẽ bị xoá\n# fileno `3` sẽ có thể được sử dụng lại khi process mở một resource khác.\npasswd.close\n\nnull = File.open('/dev/null')\nputs null.fileno\n =\u003e 3\n```\n\nChúng ta có thể thắc mắc ở đây\n\u003e `fileno` sẽ luôn là số nhỏ nhất mà chưa được sử dụng\n \nở trên chỉ thấy `fileno` là `3` là nhỏ nhất, vậy `0`, `1`, `2` đi đâu rồi?\n\nCâu trả lời là, mỗi process khi chạy đều sẽ mở sẵn 3 resources. `STDIN`, `STDOUT`, `STDERR`\n\n```ruby\nputs STDIN.fileno\n =\u003e 0\nputs STDOUT.fileno\n =\u003e 1\nputs STDERR.fileno\n =\u003e 2\n```\nĐây là 3 resources tiêu chuẩn của mỗi process.\n- `STDIN` (input) là để nhận input từ keyboard, pipes (nhận user input từ bàn phím)\n- `STDOUT` (output) là để process có thể ghi output ra ngoài devices, (in kết quả ra terminal)\n- `STDERR` (error) là để process có thể ghi lỗi ra bên ngoài (ghi lỗi ra file log chẳng hạn)\n\n### Fork process\n\nĐể tạo một process con từ một process cha, UNIX cung cấp cho chúng ta một API `fork`\n\nProcess con sau khi fork từ process cha sẽ giống hệt process cha từ memory đến file descriptor.\n\nNhư đoạn đầu đã giới thiệu, process con sẽ có `ppid` là `pid` của process cha.\n\n```ruby\n# In ra process id hiện tại\nputs \"Parent process ID: #{Process.pid}\"\n# =\u003e Parent process ID: 99719\n\n# fork một child process\nfork do\n\t# In ra process id của child process\n\tputs Process.pid\n\t# =\u003e 99762\n\n\t# In ra parent process id của child process\n\tputs Process.ppid\n\t# =\u003e 99719\nend\n```\n\nProcess con sẽ thừa kế lại toàn bộ file descriptor đang mở của process cha. `fileno` cũng sẽ không hề thay đổi. Vì vậy nên process con có thể dùng chung file, sockets với process cha.\n\nProcess con cũng sẽ thừa kế toàn bộ memory của process cha.\n\nVí dụ, chúng ta có một app Rails trong RAM 500MB, thì khi fork một process, process con cũng sẽ chiếm thêm 500MB memory copy từ process cha.\n\nSử dụng `fork` cho phép chúng ta có thể tạo thêm nhiều App instances trong tích tắc.\n\nNhưng như đã nói ở trên, việc copy sẽ rất tốn kém về RAM.\n\nUNIX optimize nhược điểm trên bằng cơ chế copy-on-write (CoW), sau khi `fork`, nếu process cha hoặc process con chưa modify lại data thì việc copy sẽ được delay, và chỉ được thực hiện khi process cha hoặc process con modify.\n\n```ruby\narr = [1, 2, 3]\n\nfork do\n\t# Bên trong process con\n\t# Tại thời điểm này, process con chưa modify `arr` nên nó sẽ tiếp tục đọc shared data từ process cha, không copy data\n\tputs arr\nend\n```\n\n```ruby\narr = [1, 2, 3]\nfork do\n\t# giống như ở phía trên, chưa có quá trình copy nào xảy ra cả\n\tarr \u003c\u003c 4\n\t# Dòng code trên modify `arr` nên một bản copy của `arr` sẽ được tạo trong memory, và process con sẽ đọc từ đó, process cha vẫn tiếp tục đọc từ vùng nhớ của riêng nó, không liên quan đến process con.\nend\n```\n\n### Orphaned Processes\nChuyện gì sẽ xảy ra nếu sau khi `fork` một process con, và trong khi process con đó đang chạy thì process cha bị kill? \n\n```ruby\n# test_orphan.rb\n\nfork do\n\t5.times do\n\t\tsleep 1\n\t\tputs \"I'm an orphan\"\n\tend\nend\n\nabort \"Parent process dies...\"\n```\n\nkhi chạy file trên ở terminal bằng `ruby test_orphan.rb`, ta sẽ thấy parent process sẽ dừng ngay lập tức, nhưng mà process vẫn còn chạy tiếp, process con vẫn còn tiếp tục ghi output ra `STDOUT` (terminal)\n\nKhi một parent process die, OS sẽ để nguyên process con, không làm gì nó cả, nó vẫn sẽ tiếp tục chạy.\n\nVậy, orphan child process thì có tác dụng gì?\n\n- Daemon process: thông thường khi ta muốn chạy một long running processes như: database server hay web server, chúng ta sẽ chạy ở chế độ daemon. Daemon process đơn giản là process được chủ ý cho trở thành orphan process.\n- Vậy làm sao chúng ta có thể giao tiếp với orphan process? ta có thể giao tiếp với orphan process bằng UNIX signals.\n\n### Process.wait\n\nTừ những ví dụ ở trên, ta có thể thấy process cha sau khi fork sẽ chạy song song với process con, và sẽ xảy ra những case như trên, khi process cha đã exit nhưng process con vẫn còn chạy.\n\nVậy làm sao process cha có thể quản lý được các process con?\n\nTrong Ruby, chúng ta có thể sử dụng `Process.wait` ở proces cha để có thể chờ cho process con exit.\n\n```ruby\nfork do\n\t5.times do\n\t\tsleep 1\n\t\tputs \"Child process\"\n\tend\nend\n\nProcess.wait\nabort \"Parent process exits...\"\n```\n\nOutput:\n```console\nChild process\nChild process\nChild process\nChild process\nChild process\nParent process exits...\n```\n\n`Process.wait` là blocking call, nghĩa là process cha sẽ dừng lại để chờ cho một process con exit, sau đó nó sẽ tiếp tục công việc của mình.\n\n**Chờ nhiều process con???**\n\nDo `Process.wait` chỉ chờ duy nhất 1 process con exit nên khi có nhiều process con, ta phải gọi `Process.wait` tương ứng với số process con đang chạy\n\n```ruby\n5.times do\n\tfork do\n\t\tsleep 1\n\t\tputs \"Child process\"\n\tend\nend\n5.times do\n\tProcess.wait\nend\n\nputs \"Parent process exits...\"\n```\n\n#### Race condition\n\n```ruby\n# Fork ra 2 process con\n2.times do\n\tfork do\n\t\t# Process con exit ngay lập tức\n\t\tabort \"Children process finished\"\n\tend\nend\n\n# Process cha chờ process con đầu tiên exit, sau đó sleep 5s\n# Trong khi process cha đang sleep thì process con thứ 2 exit.\nputs Process.wait\nsleep 5\n\n# Thông tin về process thứ 2 vẫn còn lúc sau đó\n# OS đã lưu lại info của process ngay cả sau khi nó exit.\nputs Process.wait\n```\n\nCó thể thấy, OS sẽ lưu lại info của process con và trả về cho process cha, ngay cả khi process con đã exit rất lâu.\n\n### Zoombie process\n\nViệc lưu lại info của process con và trả về cho process cha có một vấn đề, đó là mặc dù process con đã kết thúc từ rất lâu, nhưng nếu process cha không gọi `Process.wait` thì info về trạng thái của process con sẽ tồn tại mãi, khiến cho việc sử dụng tài nguyên của OS không hiệu quả.\nVà process con sẽ trở thành `Zoombie process`.\n\nChúng ta có thể sử dụng `Process.detach(pid)` để có clean up.\n\n`Process.detach` đơn giản chỉ tạo ra một thread và làm một việc đơn giản là chờ process con đó kết thúc.\n\n[Process.detach](https://github.com/namtx/til/issues/148)","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/Working-with-Processes":{"title":"Working with Processes","content":"\n### Working with Processes\n\n#### Primer\n\n##### System calls\n\nThe kernel of your Unix system sits atop the hardware of your computer.\n\nIt’s the middle man for any interactions that need to happen with the hardware.\n\nThis include things like writing/reading from the filesystem, sending data over the network, allocating memory, or playing audio over the speakers.\n\nGiven its power, programs are not allowed direct access to the kernel. Any communication is done via system calls.\n\n![[user application.png]]\n\nSystem calls are at the heart of C programming.\n\nSystem calls allow you user-space programs to interact indirectly with the hardware of your computer, via the kernel.\n\n\n##### Processes: The atoms of Unix\n\nProcesses are the building block of a Unix system. Why? Because any **code that executed happens inside a process.**\n\nOne process can spawn and manage many others.\n\n#### Processes have IDs\n\nEvery process running on your system has a unique process identifier. Hereby referred to as `pid`\n\n```ruby\nputs Process.pid\n```\n\n```shell\n$ irb\n2.7.2 :001 \u003e Process.pid\n =\u003e 69625\n2.7.2 :002 \u003e\n```\n\n```shell\n$ ps aux | grep irb\nnamtx            69633   0.0  0.0 34122844    864 s005  S+   12:55PM   0:00.01 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn --exclude-dir=.idea --exclude-dir=.tox irb\nnamtx            69625   0.0  0.1 34249756  22564 s002  S+   12:54PM   0:00.79 irb\n\n```\n\n##### Sytem calls\nRuby's `Process.pid` maps to `getpid(2)`\nThere is alos a global variable that holds the value of the current `pid`. You can access it with `$$`.\n```shell\n$ irb\n2.7.2 :001 \u003e $$\n =\u003e 69929\n2.7.2 :002 \u003e\n```\n\n#### Processes have parents\nevery process running on your system has a parent process. Each process knows its parent process identifier `ppid`.\nIn the majority of cases the parent process for a given process is the process that invoked it.\n\n```shell\n$ irb\n2.7.2 :001 \u003e\n2.7.2 :001 \u003e Process.ppid\n =\u003e 69398\n2.7.2 :002 \u003e\n```\n\n```shell\n$ ps aux | grep 69398\nnamtx            70430   0.0  0.0 34122844    864 s003  S+    1:01PM   0:00.01 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn --exclude-dir=.idea --exclude-dir=.tox 69398\nnamtx            69398   0.0  0.1 34191384   8408 s005  Ss   12:54PM   0:02.76 -zsh\n```\n\n##### In the real world\nThere aren't a ton of uses for the `ppid` in the real world. It can be important when detecting deamon processes.\n\n##### System calls\nRuby's `Process.ppid` maps to `getppid(2)`.\n\n#### Processes Have File Descriptors\n##### Everything is a File\nA part of the Unix philosophy: in the land of Unix `everything is a file`.\nThis means that devices a treated as files, sockets and pipes are treated as files. and files are treated as files.\n\n##### Descriptors represent resources\nAnytime that you open a resource in a running process it is assigned a file descriptor number. File descriptors are not shared between unrelated processes, they live and die with the process they are bound to, just as any open resources for a process are close when it exits.\n\nIn Ruby, open resources are presented by the `IO` class. Any `IO` object can have an associated file descriptor number. Use `IO#fileno` to get access to it.\n\n```ruby\npasswd = File.open('/etc/passwd')\nputs passwd.fileno\n3\n```\n\n- File descriptor numbers are assigned the lowest unused value.\n- Once a resource is closed its file descriptor number becomes available again.\n\nFile descriptors keep track of open resources only. Closed resources are not given a file descriptor number.\n\nTrying to read file descriptor number from closed resource will raise ans exception:\n\n```ruby\npasswd = File.open('/etc/passwd')\nputs passwd.fileno \n3\npasswd.close \nputs passwd.fileno\n-e:4:in `fileno': closed stream (IOError)\n```\n\nYou may have notices that when we open a file and ask for its file descriptor number, the lowest value we get is 3. What happend to 0, 1 and 2?\n\n##### Standard Streams\nEvery Unix process comes with three open resources. These are your standard input (`STDIN`), standard output (`STDOUT`) and standard error (`STDERR`) resources.\n\nThese standard resources exits for a very important reason that we take for granted today. `STDIN` provides generic way to read input from keyboard devices or pipes, `STDOUT` and `STDERR` provide generic way to write output to monitors, files, printers, etc. This was one of innovations of Unix.\n\n```ruby\nputs STDIN.fileno\n0\nputs STDOUT.fileno\n1\nputs STDERR.fileno\n2\n```\n\n##### In the real world\nFile descriptors are at the core of network programming using sockets, pipes, etc. and are also at the core of any file system descriptors.\n\n#### Processes Have Resource Limit\n##### finding the limit\n```ruby\np Process.getrlimit(:NOFILE)\n# [256, 9223372036854775807] // [soft_limit, hard_limit]\n```\n\nThe soft limit isn't really a limit. Meaning that if you exceed the soft limit (in this case by opening more than 256 resouces at once) an exception will be raised, but you can always change that limit if you want.\n\n\u003e hard_limit == Process::RLIM_INFINITY\n\nSo any process is able to change its own soft limit, but only superuser can change the hard limit.\n\nHowever, you process is also able to bump the hard limit assuming it has the required permissions.\n\nIf you interested in changing the limit at a system-wide level then start by having a look at `sysctl(8)`.\n\n##### Bump the Soft limit\n\n```ruby\nProcess.setrlimit(:NOFILE, 4096)\np Process.getrlimit(:NOFILE)\n# [4096, 4096]\n```\n\nYou can see that we set the new limit for the number of open files, and upon asking for that limit again both the hard limit and the soft limit were set to the new value `4-96`.\n\n##### Exceeding the limit\n\n```ruby\nProcess.setrlimit(:NOFILE, 3)\nFile.open('/dev/null')\n# Errno::EMFILE: Too many open files - /dev/null\n```\n\n##### Other resources\n\n```ruby\n# The maximum number of simultaneous processes \n# allowed for the current user. \nProcess.getrlimit(:NPROC) \n# The largest size file that may be created. \nProcess.getrlimit(:FSIZE) \n# The maximum size of the stack segment of the \n# process. \nProcess.getrlimit(:STACK)\n```\n\n##### In the real world\n\nNeeding to modify limits for system resources isn't a common need for most programs.\n\n`httperf(1)` is the http performance tool, and it has to change the soft limit when does something like:\n`httperf --hog --server www --num-conn 5000`\n\n#### Processes Have an Environment\n- environment variables\n\n- Every process inherits ENVs from its parent. They are set by parent process and inherited by its children processes. Environment variables are per-process and are global to each process.\n\n##### It's a hash, right?\nAlthough `ENV` uses the hash-style accessor API, it's not actually a `Hash`. For instance, it implements `Enumerable` and some of `Hash` API, but not all of it.\n\n##### System calls\n- `setenv(3)`\n\n- `getenv(3)`\n\n- `environ(7)`\n\n#### Processes Have Arguments\n\nEvery process has access to a special array called `ARGV`. Other programming languages may implement it slightly differently, but every one has something called `argv`.\n`argv` is a short form for `argument vector`. In other words: a vector, or array, of arguments.\n\n##### It's an Array!\n\n`ARGV` is simply an `Array`. You can add elements to it, remove elements from it, change the element it contains, whatever you like.\n\nSome libraries will read from `ARGV` to parse command line optinos, for example. You can programmatically change `ARGV` before they have a chance to see it in order to modify the options at runtime.\n\n##### In the real world\n- file names\n- parsing command line input. There are many Ruby libraries for dealing with command line input. One called `optparse` is available as part of the standard library.\n\n#### Processes Have Names\n\nUnix processes have very few inherent ways of communicating about their state.\nProgrammers have worked around this and invented things like `logfiles`.\n`Logfiles` allow processes to communicate anything they want about their state by writing to the filesystem, but this operates at the level of the filesystem rather than being inherent to the process itself.\n\nSimilarly, processes can use the network to open sockets and communicate with other processes. But again, that operates at a different level than the process itself, since it relies on the network.\n\nThere are two mechanisms that operate at the level of process itself that can be used to communicate information. One is the process name, the other is exit codes.\n\n##### Naming Processes\n\nEvery process on the system has a name.\n\n- `$PROGRAM_NAME` variable in Ruby\nYou can assign a value to ther global variable to change the name of current process.\n\n\n#### Processes Have Exit Codes\n\nWhen a process comes to the end of it has one last chance to make its mark on the world: its `exit code`.\nEvery process that exits does so with a numeric exit code (0-255) denoting whether it exited successfully or with an error.\n\nTraditionally, a process that exits with an `exit code 0 is said to be succesful`.\nAny other exit code denotes an error, with different codes poiting to different errors.\n\nIt's usually a good idea to stick with the `0 as success` exit code tradition so that your program will play nicely with other Unix tools.\n\n##### How to exit a Process\n\n- `exit`, `Kernel#exit`\n\n```ruby\nexit\n```\n\nYou can pass a custom exit code to this method\n\n```ruby\nexit 22\n```\n\n```ruby\n# When Kernel#exit is invoked, before exiting Ruby invokes any blocks \n# defined by Kernel#at_exit. \nat_exit { puts 'Last!' }\nexit\n```\n\nwill output:\n\n```console\nLast!\n```\n\n- `exit!`\n\nis almost exactly the same as `Kernel#exit`, but with two key differences. The first is that it sets unsuccessful status code by default (1), and the second is that it will not invoke any blocks defined using `Kernel#at_exit`\n\n- `abort`\n\n`Kernel#abort` provides a generic way to exit a process unsuccessful. `Kernel#abort` will set the exit code to `1` for the current process.\n\n```ruby\n# Will exit with exit code 1. \nabort\n\n# You can pass a message to Kernel#abort. This message will be printed \n# to STDERR before the process exits. \nabort \"Something went horribly wrong.\"\n\n# Kernel#at_exit blocks are invoked when using Kernel#abort. \nat_exit { puts 'Last!' } \nabort \"Something went horribly wrong.\"\n\nSomething went horribly wrong. \nLast!\n```\n\n- `raise`\n\nA different way to end a process is with an unhandled exception.\n\nEnding process this way still invoke any `at_exit` handlers and will print the exception message and backtrace to `STDERR`\n\n```ruby\n# Similar to abort, an unhandled exception will set the exit code to 1. \nraise 'hell'\n```\n\n#### Processes Can Fork\n\nForking is one of the most powerful concepts in Unix programming.\nThe `fork(2)` system call allows a running process to create new process programmatically. This new process is en exact copy of the original process.\nWhen forking, the process that initiates the `fork(2)` is called the `parent`, and the newly created process is called the `child`.\n\n\u003e The child process inherits a copy of all of the memory in use by the parent proces, as well as open file descriptors belongings to the parent process.\n\nSince the child process is an entirely new process, it gets its own unique `pid`\n\nThe parent of the child process is, obviously, its parent process. So its `pid` is set to the pid of the processs that initiated the `fork(2)`.\n\nThe child proces inherits any open file descriptors from the parent process at the time of `fork(2)`. It's given the same map of the file descriptor numbers that the parent process has. In this way, the two processes can share open files, sockets, etc.\n\nThe child process inherits copy of everything that the parent process has in the main memory. In this way a process could loaded up a large code base, say a Rails app, that occupies 500MB of main mermory. Then this process can fork 2 new child processes. Each of these child processes effectivly have their own copy of that code base loaded in memory.\n\nThe call to `fork` returns nearly-instantly so we have 3 processes with each using 500MB of memory. Perfect for when you want to have multiple instances of your application loaded in memory at the same time. Because, only once process needs to load the appl and forking is fast, this method is faster than loading the app 3 times in separate instances.\n\nThe child processes would be free to modify their copy of the memory without effecting what the parent process has in memory.\n\n```ruby\nif fork\n\tputs \"entered the if block\"\nelse\n\tputs \"entered the else block\"\nend\n\n# entered the if block \n# entered the else block\n```\n\nOne call to ther `fork` method actually returns twice. Remember that `fork` create a new process. So it returns once in the calling process parent and once in the newly created process child.\n\n```ruby\nputs \"parent process pid is #{Process.pid}\"\n\nif fork \n\tputs \"entered the if block from #{Process.pid}\"\nelse\n\tputs \"entered the else block from #{Process.pid}\"\nend\n```\n\noutput:\n```console\nparent process is 21268\nentered the if block from 21268\nentered the else block from 21282\n```\n\nNow it becomes clear that the code in the if block is being excuted by the parent process, while the code in the else block is being excuted by the child process. Both the child process and the parent process will carry on excuting the code after the if construct.\n\n**In the child process `fork` return `nil`.  In the parent process `fork` returns the pid if the newly created child process.**\n\n```ruby\nputs fork\n21423\nnil\n```\n\n\n##### Multicore programming?\nBy making new process it means that your code is able, but not guaranteed, to be distributed accross multiple CPU cores.\n\nHowever, there's no guarantee that stuff will be happening in parallel. On a busy system it's possible that all 4 of your processes are handled by the same CPU.\n\n\u003e fork(2) creates a new process that's a copy of the old process. So if a process is using 500MB of main memory, then it forks, now you have 1GB in main memory.\n\u003e \n\u003e Do this another ten times and you can quickly exhaust main memory. This is often called a _fork bomb_. Before you turn up the concurrency make sure that you know the consequences.\n\n##### Using a Block\n\nIn the example above we've demonstrated `fork` with an if/else construct. It's also possible, and more common in Ruby code, to use `fork` with a block.\n\nWhen you pass a block to the `fork` method that block will be execute in the new child process, while the parent process simply skips over it. The child process exits when it's done excuting the block. It doesn not continue along the same code path as the parent.\n\n```ruby\nfor do\n  # Code here is only executed in the child process\nend\n\n# Code here is only executed in the parent process\n```\n\n#### Orphaned Processes\n```ruby\nfork do\n\t5.times do\n\t\tsleep 1\n\t\tputs \"I'm an orphan\"\n\tend\nend\n\nabort \"Parent process die...\"\n```\n\n\n##### Abandoned Children\n\nWhat happens to a child process when its parent dies?\n\n_Nothing_, that is to say, the OS doesn't treat child processes any differently than any other processes. So, when the parent process dies the child process continues on; the parent process does not take the child down with it.\n\n##### Managing Orphans\n\n- `Daemon processes` are long running processes that are intentionally orphaned and meant to stay running forever.\n- Communicationg with processes that are not attached to a terminal session. You can do this using something \n\n#### CoW\nAs metioned, `fork(2)` creates a new child process that's an exact copy of the parent process. This includes a copy of everything the parent process has in memory.\n\nPhysically copying all of data can be considerable overhead, so modern Unix systems employ something called `copy-on-write` sematics (CoW) to combat this.\n\nCoW delays the actual copying on memory until it needs to be written.\n\nSo the parent process and child process will actually share the same physical data in memory until one of them need to modify it, at which point the memory will be copied so the proper separation between the two processes can be preserved.\n\n```ruby\narr = [1,2,3] fork do\n# At this point the child process has been initialized. \n# Using CoW this process doesn't need to copy the arr variable, \n# since it hasn't modified any shared values it can continue reading \n# from the same memory location as the parent process. \n\tp arr\nend\n```\n\n```ruby\narr = [1,2,3] fork do \n# At this point the child process has been initialized. \n# Because of CoW the arr variable hasn't been copied yet. \n\tarr \u003c\u003c 4 \n# The above line of code modifies the array, so a copy of \n# the array will need to be made for this process before \n# it can modify it. The array in the parent process remains \n# unchanged. \nend\n```\n\n\n\u003e MRI's garbage collector uses a `mark-and-sweep` algorithm. In a nutshell this means that when the GC is invoked it must traverse the graph of live objects, and for each one the GC must `mark` it as alive.\n\u003e \n\u003e In MRI \u003c= 1.9, this `mark` step was implemented as a modification to that object in memory. So when the GC was invoked right after a `fork`, all live objects were modified, forcing the OS to make copies of all live Ruby objects and foregoing any benefit from CoW semantics.\n\u003e \n\u003e MRI \u003e= 2.0 still uses a `mark-and-sweep` GC, but preserves CoW semantics by storing all of the `marks` in a small data structure in a disparate region of memory. So when the GC runs after a `fork`, this small region of memory must be copied, but the graph of live Ruby objects can be shared between parent and child until your code modifies an object.\n\n#### Processes can wait\n- _fire and forget_ is useful when you want a child process to handle something asynchrously, but the parent process still has its own work to do.\n\n```ruby\nmessage = 'Good Morning'\nrecipient = 'tree@mybackyard.com'\n\nfork do \n\t# In this contrived example the parent process forks a child to take \n\t# care of sending data to the stats collector. Meanwhile the parent \n\t# process has continued on with its work of sending the actual payload. \n\t# The parent process doesn't want to be slowed down with this task, and \n\t# it doesn't matter if this would fail for some reason. \n\tStatsCollector.record message, recipient\nend\n# send message to recipient\n```\n\n- `Process.wait`\n\n```ruby\nfork do \n\t5.times do\n\t\tsleep 1 \n\t\tputs \"I am an orphan!\"\n\tend \nend\nProcess.wait\nabort \"Parent process died...\"\n```\n\n```console\nI am an orphan! \nI am an orphan! \nI am an orphan! \nI am an orphan! \nI am an orphan! \nParent process died...\n```\n\nControl will not be returned to the terminal until all of the output has been printed.\n\n**`Process.wait` is a blocking call instructing the parent process to wait for one of its child processes to exit before continuing.**\n\n```ruby\n# We create 3 child processes. \n3.times do \n\tfork do \n\t\t# Each one sleeps for a random amount of number less than 5 seconds. \n\t\tsleep rand(5) \n\tend\nend \n3.times do \n\t# We wait for each child process to exit and print the pid that \n\t# gets returned. \n\tputs Process.wait \nend\n```\n\n##### Race Conditions\n```ruby\n# We create two child processes. \n2.times do \n\tfork do \n\t\t# Both processes exit immediately. \n\t\tabort \"Finished!\" \n\tend \nend \n# The parent process waits for the first process, then sleeps for 5 seconds. \n# In the meantime the second child process has exited and is no \n# longer running. \nputs Process.wait \nsleep 5 \n# The parent process asks to wait once again, and amazingly enough, the second # process' exit information has been queued up and is returned here. \nputs Process.wait\n```\n\n\nThe kernel queues up information about exited processes so that parent always receives the information in order that children exited.\n\n##### In the real World\n- _babysitting processes_\n\nAt the core of this pattern is the concept that you have one process that forks serveral child processes, for concurrency, and then spends ti\n\n- `waitpid(2)`\n\n#### Zoombie Process\n\n- `fire and forget manner`\n\nKernel queues up information about child processes that have exited. So even if you `Process.wait` long after process has exited its information is still available.\n\nThe kernel will retain the status of exited child process until the parent process requests that status using `Process.wait`.\nIf the parent never requests the status t hen the kernel will never _reap_ that status information. So creating fire and forget child process without collecting their status information is poor use of kernel resources.\n","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/gRPC":{"title":"gRPC","content":"\nhttps://developers.google.com/protocol-buffers/docs/proto3#updating","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/gRPC-CLI-cheatsheet":{"title":"gRPC CLI cheatsheet","content":"\n### ls\n```\ngrpc_cli ls localhost:9090\n```\n","lastmodified":"2022-12-31T10:33:01.738873034Z","tags":null},"/notes/memtable":{"title":"memtable","content":"","lastmodified":"2022-12-31T10:33:01.762874068Z","tags":null},"/notes/read-after-write":{"title":"read-after-write","content":"\nRead-after-write consistency is **the ability to view changes (read data) right after making those changes (write data)**. For example, if you have a user profile and you change your bio on the profile, you should see the updated bio if you refresh the page. There should be no delay during which the old bio shows up.","lastmodified":"2022-12-31T10:33:01.762874068Z","tags":null}}